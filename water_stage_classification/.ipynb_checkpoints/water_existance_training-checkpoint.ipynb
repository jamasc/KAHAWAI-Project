{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16122ee7-0e2b-45f3-be79-833d0b478124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### This code is for binary classification (water / no water) in water streams\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "## Binary Streamflow Dataset\n",
    "# labels are converted to 1 (water) and 0 (no water), the last two labels ('poor quality' and 'not working') are discarded\n",
    "binary_class_map = {0:1, 1:0, 2:0, 3:1}\n",
    "class BinaryStreamFlowDataset(Dataset):\n",
    "    def __init__(self, excel_file, transform=None):\n",
    "        self.data = []\n",
    "        xls = pd.ExcelFile(excel_file)\n",
    "\n",
    "        # each sheet = one label\n",
    "        for idx, sheet_name in enumerate(xls.sheet_names):\n",
    "            if idx > 3: break\n",
    "            label = binary_class_map[idx]\n",
    "            df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "            for img_path in df['Image_Path'].dropna():\n",
    "                img_path = img_path.replace('D:', '../images')\n",
    "                img_path = img_path.replace('\\\\', '/')\n",
    "                self.data.append((img_path, label))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# dataset = BinaryStreamFlowDataset('../data/image_inventory_cam_1000.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83c3428-94ce-4596-8ee3-3f6430d2c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard preprocessing for ResNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "    ),\n",
    "])\n",
    "\n",
    "dataset = BinaryStreamFlowDataset(\"../data/image_inventory_cam_1000.xlsx\", transform=transform)\n",
    "\n",
    "## Split\n",
    "train_size = int(0.7 * len(dataset))   # 70%\n",
    "val_size   = int(0.15 * len(dataset))  # 15%\n",
    "test_size  = len(dataset) - train_size - val_size  # remaining 15%\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e207f9-cff0-47f8-b76e-bbd98b88ab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jans26/.conda/envs/streamflow-env/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jans26/.conda/envs/streamflow-env/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pretrained ResNet-50\n",
    "embedder = models.resnet50(pretrained=True)\n",
    "embedder.fc = nn.Identity()    #output embeddings\n",
    "embedder.eval()\n",
    "\n",
    "# Small classification head\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(2048, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "\n",
    "    nn.Linear(256, 32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "\n",
    "    nn.Linear(32, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100940ad-ad0b-4667-81f9-def5520fadce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-glitter-6</strong> at: <a href='https://wandb.ai/jans26-university-of-hawaii-system/resnet50-classifier/runs/kl0o9tc2' target=\"_blank\">https://wandb.ai/jans26-university-of-hawaii-system/resnet50-classifier/runs/kl0o9tc2</a><br> View project at: <a href='https://wandb.ai/jans26-university-of-hawaii-system/resnet50-classifier' target=\"_blank\">https://wandb.ai/jans26-university-of-hawaii-system/resnet50-classifier</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250902_023125-kl0o9tc2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/lustre/koa/scratch/jans26/streamflow/code/wandb/run-20250902_025202-mns60n18</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jans26-university-of-hawaii-system/water_existence/runs/mns60n18' target=\"_blank\">test run 9/1</a></strong> to <a href='https://wandb.ai/jans26-university-of-hawaii-system/water_existence' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jans26-university-of-hawaii-system/water_existence' target=\"_blank\">https://wandb.ai/jans26-university-of-hawaii-system/water_existence</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jans26-university-of-hawaii-system/water_existence/runs/mns60n18' target=\"_blank\">https://wandb.ai/jans26-university-of-hawaii-system/water_existence/runs/mns60n18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  68%|██████▊   | 216/316 [16:02<07:24,  4.44s/it, loss=0.137] "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Initialize Weights & Biases ---\n",
    "wandb.init(\n",
    "    project=\"water_existence\",\n",
    "    name=\"test run 9/1\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 32,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    }\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedder.to(device)\n",
    "classifier.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "def get_embeddings(images):\n",
    "    with torch.no_grad():\n",
    "        feats = embedder(images)  # [batch, 2048, 1, 1]\n",
    "        feats = feats.view(feats.size(0), -1)  # flatten -> [batch, 2048]\n",
    "    return feats\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    # Training\n",
    "    classifier.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        embeddings = get_embeddings(images)\n",
    "        outputs = classifier(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_loss = total_loss / total\n",
    "\n",
    "    # Validation\n",
    "    classifier.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            embeddings = get_embeddings(images)\n",
    "            outputs = classifier(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    val_loss = val_loss / val_total\n",
    "\n",
    "    # --- Logging ---\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss {train_loss:.4f}, Train Acc {train_acc:.4f}, \"\n",
    "          f\"Val Loss {val_loss:.4f}, Val Acc {val_acc:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc\n",
    "    })\n",
    "\n",
    "# Saving model (classification head)\n",
    "torch.save(classifier.state_dict(), 'water_existence_classifier_binary_9_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e74d9-172f-4655-bffb-d1503c1ac008",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test loop (standalone)\n",
    "\n",
    "test_loss, test_correct, test_total = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        embeddings = get_embeddings(images)\n",
    "        outputs = classifier(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "test_loss = test_loss / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398c922-b608-4ae9-b531-ef3933c88f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
